?mtcars
plot(mtcars$mpg~mtcars$am)
plot(mtcars$mpg~factor(mtcars$am))
plot(mtcars$mpg~factor(mtcars$am), xlab=c("Auto","Mannual"))
factor(mtcars$am)
factor(mtcars$am, level =c("Auto","Mannual"))
factor(mtcars$am, label =c("Auto","Mannual"))
plot(mtcars$mpg~factor(mtcars$am, label =c("Auto","Mannual")), xlab=c("Auto","Mannual"))
plot(mtcars$mpg~factor(mtcars$am, label =c("Auto","Mannual")))
plot(mtcars$mpg~factor(mtcars$am, label =c("Auto","Mannual")),xlab="")
xlab="", ylab="Fuel Economy (MPG)")
plot(mtcars$mpg~factor(mtcars$am, label =c("Auto","Mannual")),
xlab="", ylab="Fuel Economy (MPG)")
plot(mtcars$mpg~factor(mtcars$am, label =c("Auto","Mannual")),
xlab="", ylab="Fuel Economy (MPG)", main="Fuel Economy on Transmission")
t.test(mtcars$mpg~factor(mtcars$am, label =c("Auto","Mannual")))
plot(cars)
head(mtcars)
plot(wt,mpg,data=mtcars)
plot(mtcars$wt,mtcars$mpg)
plot(mtcars$mpg~factor(mtcars$am, label =c("Auto","Mannual")),
xlab="", ylab="Fuel Economy (MPG)",
main="Fuel Economy on Transmission")
plot(mtcars$wt,mtcars$mpg)
fit1 <- lm(mpg~factor(am)-1)
fit1 <- lm(mpg~factor(am)-1, data=mtcars)
summary(fit1)
fit1 <- lm(mpg~factor(am, label=c("Auto","Mannual")-1, data=mtcars)
fit1 <- lm(mpg~factor(am, label=c("Auto","Mannual"))-1, data=mtcars)
summary(fit1)
fit2 <- lm(mpg~factor(am, label=c("Auto","Mannual"))+wt-1, data=mtcars)
summary(fit2)
anova(fit1,fit2)
plot(resid(fit1))
plot(resid(fit2))
plot(resid(fit2),resid(fit1))
plot(fit1)
plot(fit2)
save.image("D:/Coursera/Data Science Certificate JHU/Regression/.RData")
attach(mtcars)
ls()
library(MASS)
?shuttle
shuttle
head(shuttle)
table(shuttle$use)
table(shuttle$wind)
glm(use~wind,data=shuttle)
glm(use~wind,data=shuttle, family = "binomial")
factor(shuttle$wind, ref="tail")
glm(use~relevel(wind,ref="tail"),data=shuttle, family = "binomial")
glm(use~relevel(wind,ref="tail")+magn,data=shuttle, family = "binomial")
glm(use~relevel(wind,ref="tail")+magn,data=shuttle, family = "binomial")
exp(0.03181)
glm(use~relevel(wind,ref="tail"),data=shuttle, family = "binomial")
f1 <- glm(use~relevel(wind,ref="tail"),data=shuttle, family = "binomial")
coef(f1)
exp(coef(f1))
f1 <- glm(use~relevel(wind,ref="tail")-1,data=shuttle, family = "binomial")
f1
exp(coef(f1))
exp(coef(f1))[2]exp(coef(f1))[1]
exp(coef(f1))[2]/exp(coef(f1))[1]
exp(coef(f1))[1]/exp(coef(f1))[2]
f1 <- glm(use~wind-1,data=shuttle, family = "binomial")
f1
shuttle$use
numeric(shuttle$use)
as.numeric(shuttle$use)
head(shuttle$use)
levels(shuttle$use)
factor(shuttle$use,label=c(1,0))
f1 <- glm(factor(shuttle$use,label=c(1,0))~wind-1,data=shuttle, family = "binomial")
f1
exp(coef(f1))[1]/exp(coef(f1))[2]
plot(f1)
f2 <- glm(factor(shuttle$use,label=c(1,0))~wind+magn-1,data=shuttle, family = "binomial")
exp(coef(f2))[1]/exp(coef(f2))[2]
f2
exp(coef(f2))[2]/exp(coef(f2))[1]
f3 <- glm(I(1-factor(shuttle$use,label=c(1,0)))~wind+magn-1,data=shuttle, family = "binomial")
f3 <- glm(factor(shuttle$use,label=c(0,1))~wind+magn-1,data=shuttle, family = "binomial")
coef(f2)
coef(f3)
factor(shuttle$use,label=c(0,1))
1-factor(shuttle$use,label=c(1,0))
1- shuttle$use
f3 <- glm(I(1-as.numeric(factor(shuttle$use,label=c(0,1))))~wind+magn-1,data=shuttle, family = "binomial")
1-as.numeric(factor(shuttle$use,label=c(0,1))
)
as.numeric(factor(shuttle$use,label=c(0,1))
)
f3 <- glm(I(2-as.numeric(factor(shuttle$use,label=c(0,1))))~wind+magn-1,data=shuttle, family = "binomial")
coef(f3)
coef(f2)
exp(coef(f3))[2]/exp(coef(f3))[1]
exp(coef(f3))[1]/exp(coef(f3))[2]
f3 <- glm(I(2-as.numeric(factor(shuttle$use,label=c(0,1))))~wind+magn,data=shuttle, family = "binomial")
f2 <- glm(factor(shuttle$use,label=c(1,0))~wind+magn,data=shuttle, family = "binomial")
coef(f2)
coef(f3)
head(InsectSprays)
f4 <- glm(count~spray -1,offset=log(count+1),family="poisson",data=InsectSprays)
f4
f5 <- glm(count~spray -1,family="poisson",data=InsectSprays)
f5
coef(f4)[1]/coef(f4)[2]
coef(f5)[1]/coef(f5)[2]
exp(coef(f4)[1])/exp(coef(f4)[2])
exp(coef(f5)[1])/exp(coef(f5)[2])
f4
f6 <- glm(count~spray -1,offset=log(count+1+log(100)),family="poisson",data=InsectSprays)
coef(f4)
coef(f6)
coef(f6)[1]/coef(f4)[1]
exp(coef(f6)[1]/coef(f4)[1])
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
plot(x,y)
log(100)
f6 <- glm(count~spray -1,offset=log(count+log(100)),family="poisson",data=InsectSprays)
f6 <- glm(count~spray -1,offset=count+log(100),family="poisson",data=InsectSprays)
f7 <- glm(count~spray -1,offset=count,family="poisson",data=InsectSprays)
coef(f6)
coef(f7)
log(100)
f6 <- glm(count~spray+offset(count+log(100)),family="poisson",data=InsectSprays)
f7 <- glm(count~spray+offset(count),family="poisson",data=InsectSprays)
log(100)
coef(f7)
coef(f6)
coef(f6)/coef(7)
coef(f6)/coef(f7)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
training <- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing <- segmentationOriginal[segmentationOriginal$Case == "Test",]
modelFit <-train(Class ~.,data=training, method="rpart")
predict(modelFit ,data.frame(VarIntenCh4  = 100, FiberWidthCh1 = 8, PerimStatusCh1=1 ))
print (modelFit$finalModel)
predict(modelFit ,data.frame(TotalIntenCh2 = 23000, FiberWidthCh1 = 10))
predict(modelFit$finalModel ,data.frame(TotalIntenCh2 = 23000, FiberWidthCh1 = 10))
print (modelFit)
predict(modelFit,data.frame(TotalIntenCh2 = 23000, FiberWidthCh1 = 10))
library(pgmm)
data(olive)
olive = olive[,-1]
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
dim(olive)
head(olive)
modelFit <-train(Area ~.,data=olive, method="rpart")
class(olive$Area)
any(is.na(olive))
newdata = as.data.frame(t(colMeans(olive)))
predict(modelFit ,newdata)
as.factor(olive[,1])
olive[,1] <- as.factor(olive[,1])
modelFit <-train(Area ~.,data=olive, method="rpart")
newdata = as.data.frame(t(colMeans(olive)))
predict(modelFit ,newdata)
newdata
predict(modelFit ,newdata[,-c(1)])
newdata[,-c(1)]
data(olive)
olive = olive[,-1]
training <- cbind(as.factor(olive[,1]),olive[,-1])
modelFit <-train(Area ~.,data=training, method="rpart")
training <- cbind(Area=as.factor(olive[,1]),olive[,-1])
head(training)
class(training)
class(training$Area)
modelFit <-train(Area ~.,data=training, method="rpart")
newdata = as.data.frame(t(colMeans(olive)))
predict(modelFit ,newdata[,-c(1)])
print(modelFit$finalmodel)
print(modelFit$finalModel)
modelFit <-train(Area ~.,data=olive, method="rpart")
print(modelFit$finalModel)
?olive
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234 )
set.seed(13234)
dim(trainSA)
colnames(trainSA)
?SAheart
trainSA = SAheart[train,c(2,3,6,7,8,9,10)]
testSA = SAheart[-train,c(2,3,6,7,8,9,10)]
modelFit <-train(chd ~.,data=trainSA, method="glm", family="binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
trainSA[,chd]
trainSA[,"chd"]
missClass(trainSA[,"chd"],predict(modelFit ,trainSA[,-c(1)]))
predict(modelFit ,trainSA[,-c(1)])
trainSA[,-c(1)]
print(modelFit$finalModel)
head(trainSA)
head(trainSA[,-1])
head(trainSA[,-7])
missClass(trainSA[,"chd"],predict(modelFit ,trainSA[,-7]))
missClass(testSA[,"chd"],predict(modelFit ,testSA[,-7]))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
dim(vowel.train)
head(vowel.train)
head(factor(vowel.train$y))
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
modelFit <-train(y ~.,data=vowel.train, method="rf", prox=T)
varImpPlot(modelFit)
varImp(modelFit)
randomForest(y ~ ., data=vowel.train,
keep.forest=FALSE, importance=TRUE)
varImpPlot(randomForest(y ~ ., data=vowel.train,
keep.forest=FALSE, importance=TRUE))
varImp(randomForest(y ~ ., data=vowel.train,
keep.forest=FALSE, importance=TRUE))
varImp(modelFit,useModel=0)
modelFit <-train(y ~.,data=vowel.train, method="rf", prox=T)
varImp(modelFit,useModel=0)
varImp(modelFit)
rf <- randomForest(y ~ ., data=vowel.train, keep.forest=FALSE, importance=TRUE)
rf$importance
rf$importance[,12:13]
im <- rf$importance[,12:13]
im
im[sort(im),]
im[sort(im[,1]),]
im[order(im[,1]),]
importance(rf)
set.seed(33833)
rf <- randomForest(y ~ ., data=vowel.train, keep.forest=FALSE, importance=TRUE)
importance(rf)
im <- rf$importance[,12:13]
im[order(im[,1]),]
set.seed(33833)
modelFit <-train(y ~.,data=vowel.train, method="rf")
varImp(modelFit)
set.seed(33833)
rf <- randomForest(y ~ ., data=vowel.train, keep.forest=FALSE, importance=TRUE)
varImp(rf)
im <- rf$importance[,12:13]
im[order(im[,1]),]
varImp(rf, useModel=0)
varImp(modelFit, useModel=0)
install.packages("pROC")
varImp(modelFit, useModel=0)
c <- varImp(modelFit, useModel=0)
c
c$cs <- colSums(c)
class(c)
as.data.frame(c)
as.matrix(c)
setwd("D:/Coursera/Data Science Certificate JHU/PracML")
rm(list=ls())
training <- read.csv("./pml-training.csv")
testing <- read.cvs("./pml-testing.csv")
library(caret)
testing <- read.csv("./pml-testing.csv")
head(training)
narm <- function(data){
for ( i in 1:nrow(data)){
data <- data[,!is.na(data[i,])]
}
return(data)
}
training <- narm(training)
testing <- narm(testing)
training <- training[,colnames(training) %in% colnames(testing)]
colnames(testing)[!colnames(testing) %in% colnames(training)]
head(testing$problem_id)
testing <- testing[,colnames(testing) %in% colnames(training)]
colnames(training)
"classe" %in% colnames(testing)
testing <- read.csv("./pml-testing.csv")
"classe" %in% colnames(testing)
sort(colnames(testing))
training <- read.csv("./pml-training.csv")
sort(colnames(training))
summary(training$classe)
testing <- read.csv("./pml-testing.csv")
sort(colnames(testing))
training <- narm(training)
testing <- narm(testing)
which(colnames(training) %in% colnames(testing))
which(colnames(training) == "classe")
training <- training[,c(93,which(colnames(training) %in% colnames(testing)))]
testing <- testing[,colnames(testing) %in% colnames(training)]
dim(training)
colnames(training)
which(colnames(training)=="classe")
preProc <- preProcess(training[,-1],method="pca",thresh=0.9)
modelFit <-train(Classe ~.,data=training, method="rf")
modelFit <-train(classe ~.,data=training, method="rf")
summary(training)
colnames(training)
data.frame(colnames(training)[-1],colnames(testing))
dim(training[,-c("X")])
dim(training[,-which(c("X"))])
dim(training[,-which(colnames(training)=="X")])
modelFit <-train(classe ~.,data=training, method="rpart")
confusionMatrix(training$classe,predict(modelFit,training[,-1]))
inTrain <- createDataPartition(y=trainraw$classe, p=0.75, list=FALSE)
rm(list-ls())
rm(list=ls())
trainraw <- read.csv("./pml-training.csv")
validation <- read.csv("./pml-testing.csv")
narm <- function(data){
for ( i in 1:nrow(data)){
data <- data[,!is.na(data[i,])]
}
return(data)
}
trainraw <- narm(trainraw)
validation <- narm(validation)
# make tidy data for both training and testing to have the same predictors
# Training set contain "classe" variable
trainraw <- trainraw[,c(93,which(colnames(trainraw) %in% colnames(validation)))]
# Exclude Data not from device
trainraw <- trainraw[,-which(colnames(trainraw) %in%
c("user_name","raw_timestamp_part_1",
"raw_timestamp_part_2","cvtd_timestamp"))]
# For validation set keep problem_id for submission
validation <- validation[,-which(colnames(validation) %in%
c("user_name","raw_timestamp_part_1",
"raw_timestamp_part_2","cvtd_timestamp"))]
inTrain <- createDataPartition(y=trainraw$classe, p=0.75, list=FALSE)
training <- trainraw[inTrain,]
testing <- trainraw[-inTrain,]
set.seed(1)
modelFit <-train(classe ~.,data=training, method="rpart")
confusionMatrix(training$classe,predict(modelFit,training[,-1]))
confusionMatrix(test,predict(modelFit,testing[,-1]))
c()
rm <- c()
c(rm,1)
c(rm,1,2)
narm <- function(data){
rm <-c()
for ( i in 1:ncol(data)){
if(any(is.na(data[,i]))){rm <- c(rm,i)}
}
return(data[,-rm])
}
trainraw <- narm(trainraw)
validation <- narm(validation)
narm <- function(data){
rm <-c()
for ( i in 1:ncol(data)){
if(any(is.na(data[,i]))){rm <- c(rm,i)}
}
return(rm)
}
narm(trainraw)
trainraw <- read.csv("./pml-training.csv")
validation <- read.csv("./pml-testing.csv")
narm(trainraw)
narm <- function(data){
rm <-c()
for ( i in 1:ncol(data)){
if(any(is.na(data[,i]))){rm <- c(rm,i)}
}
return(data[,-rm])
}
trainraw <- narm(trainraw)
validation <- narm(validation)
trainraw <- trainraw[,c(93,which(colnames(trainraw) %in% colnames(validation)))]
# Exclude Data not from device
trainraw <- trainraw[,-which(colnames(trainraw) %in%
c("user_name","raw_timestamp_part_1",
"raw_timestamp_part_2","cvtd_timestamp"))]
# For validation set keep problem_id for submission
validation <- validation[,-which(colnames(validation) %in%
c("user_name","raw_timestamp_part_1",
"raw_timestamp_part_2","cvtd_timestamp"))]
predict(modelFit,testing[,-1])
confusionMatrix(testing$classe,predict(modelFit,testing[,-1]))
modelFit <-train(classe ~.,data=training, method="lda")
confusionMatrix(training$classe,predict(modelFit,training[,-1]))
confusionMatrix(testing$classe,predict(modelFit,testing[,-1]))
colnames(validation)
predict(modelFit,validation[,-56])
nbmodelFit <-train(classe ~.,data=training, method="nb")
install.packages("klaR")
nbmodelFit <-train(classe ~.,data=training, method="nb")
library(caret)
trainraw <- read.csv("./pml-training.csv")
validation <- read.csv("./pml-testing.csv")
```
After read in the raw data, we removed any column containing missing value to ignore that predictor for better and easier handling of data.
```{r clean, cache=TRUE}
# Define Clean data function
narm <- function(data){
rm <-c()
for ( i in 1:ncol(data)){
if(any(is.na(data[,i]))){rm <- c(rm,i)}
}
return(data[,-rm])
}
trainraw <- narm(trainraw)
validation <- narm(validation)
# make tidy data for both training and testing to have the same predictors
# Training set contain "classe" variable
trainraw <- trainraw[,c(93,which(colnames(trainraw) %in% colnames(validation)))]
# Exclude Data not from device
trainraw <- trainraw[,-which(colnames(trainraw) %in%
c("user_name","raw_timestamp_part_1",
"raw_timestamp_part_2","cvtd_timestamp"))]
# For validation set keep problem_id for submission
validation <- validation[,-which(colnames(validation) %in%
c("user_name","raw_timestamp_part_1",
"raw_timestamp_part_2","cvtd_timestamp"))]
inTrain <- createDataPartition(y=trainraw$classe, p=0.75, list=FALSE)
training <- trainraw[inTrain,]
testing <- trainraw[-inTrain,]
set.seed(1)
nbmodelFit <-train(classe ~.,data=training, method="nb")
warnings()
confusionMatrix(training$classe,predict(ldamodelFit,training[,-1]))
ldamodelFit <-train(classe ~.,data=training, method="lda")
confusionMatrix(training$classe,predict(ldamodelFit,training[,-1]))
confusionMatrix(testing$classe,predict(ldamodelFit,testing[,-1]))
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
ans <- predict(ldamodelFit,validation[,-56])
pml_write_files(ans)
ans
modelFit <-train(classe ~.,data=training, method="rpart")
predict(modelFit,validation[,-56])
predict(ldamodelFit,testing[,-1])
testing[1,-1]
testing[,1]
trainraw$classe
ldamodelFit <-train(classe ~.,data=training, method="lda")
ldamodelFit <-train(classe ~.,data=training, method="lda")
ldamodelFit <-train(classe ~.,data=training, method="lda")
confusionMatrix(testing$classe,predict(ldamodelFit,testing[,-1]))
predict(ldamodelFit,validation[,-56])
confusionMatrix(testing$classe,predict(modelFit,testing[,-1]))
testing$classe
predict(modelFit,testing[,-1])
summary(testing$classe)
confusionMatrix(testing[order(testing$pitch_belt),]$classe,predict(modelFit,testing[order(testing$pitch_belt),-1]))
confusionMatrix(testing[order(testing$pitch_belt),]$classe,predict(ldamodelFit,testing[order(testing$pitch_belt),,-1]))
class(ans)
as.character(ans)
pml_write_files(as.character(ans))
rf <- randomForest(classe ~ ., data=training, keep.forest=FALSE, importance=TRUE)
library(randomForest)
rf <- randomForest(classe ~ ., data=training, keep.forest=FALSE, importance=TRUE)
predict(rf,validation[,-56])
rf <- randomForest(classe ~ ., data=training, keep.forest=T, importance=TRUE)
predict(rf,validation[,-56])
class(trainraw$classe)
trainraw$classe
as.character(trainraw$classe)
predict.randomForest(rf,validation[,-56])
library(randomForest)
predict.randomForest(rf,validation[,-56])
predict(rf,validation[,-56])
rf
importance(rf)
importance(rf)[,7]
sort(importance(rf)[,7])
predict(rf,testing[,-1])
confusionMatrix(testing$classe,predict(rf,testing[,-1])
)
data.frame(t=colnames(testing),v=colnames(validation))
data.frame(t=colnames(testing[,-1]),v=colnames(validation[,-56]))
validation[,56]
predict(rf,validation[,-56])
colnames(validation[,-56])
colnames(validation[,-56])==colnames(testing[,-1])
summary(validation[,-56])
summary(testing)
class(validation$new_window)
validation$new_window
factor(validation$new_window,levels=c("yes","no"))
head(testing$new_window)
factor(validation$new_window,levels=c("no","yes"))
validation$new_window <- factor(validation$new_window,levels=c("no","yes"))
predict(rf,validation[,-56])
BT <- testing[testing$classe=="B",]
BT
head(BT)
predict(rf,BT[,-1])
save.image("D:/Coursera/Data Science Certificate JHU/PracML/PracML.RData")
